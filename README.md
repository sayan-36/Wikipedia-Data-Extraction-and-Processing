# Wikipedia-Data-Extraction-and-Processing

This Python-based project is designed to automate the extraction, cleaning, and processing of data from Wikipedia. The extracted data is then pushed to Azure Data Lake for further analysis and processing. This project provides a robust solution for collecting and preparing diverse datasets from Wikipedia, facilitating seamless integration into downstream data processing pipelines.

Key Features:
1)Wikipedia Data Crawling:

Leverage Python's web scraping libraries, such as BeautifulSoup and requests, to efficiently crawl and extract relevant data from Wikipedia pages.

2)Data Cleaning and Preprocessing:

Implement robust data cleaning algorithms to handle inconsistencies, missing values, and outliers in the extracted Wikipedia data. This ensures the data is of high quality and ready for downstream processing.

3)Automated Processing Pipelines:

Design and implement automated data processing pipelines on Azure to handle the ingested Wikipedia data. Leverage Azure services like Azure Databricks or Azure Synapse Analytics for scalable and efficient data processing.
